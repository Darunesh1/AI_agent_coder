# ============================================
# AI Coder Agent - Environment Configuration
# ============================================
# Copy this file to .env and fill in your actual values
# cp .env.example .env

# ============================================
# REQUIRED - App Configuration
# ============================================

# Your unique secret key (create a strong password)
# This will be used to validate incoming task requests
# IMPORTANT: Match this with what you submit in the Google Form
APP_SECRET=your_secret_key_here

# GitHub Personal Access Token
# Get it from: https://github.com/settings/tokens
# Required scopes: repo, workflow
# Example: ghp_1234567890abcdefghijklmnopqrstuvwxyz12
GITHUB_TOKEN=ghp_YOUR_GITHUB_TOKEN_HERE

# ============================================
# LLM PROVIDER SELECTION
# ============================================
# Choose one: gemini, aipipe, openai, or ollama
# Recommended: gemini (fast and reliable)
LLM_PROVIDER=gemini

# ============================================
# SCENARIO 1: Google Gemini API (RECOMMENDED)
# ============================================
# Get your API key from: https://aistudio.google.com/app/apikey
# Fast, reliable, and good at code generation
GOOGLE_API_KEY=YOUR_GEMINI_API_KEY_HERE
GEMINI_MODEL=gemini-2.0-flash-exp

# Other Gemini model options:
# - gemini-2.5-flash-lite (fastest, lightweight)
# - gemini-1.5-pro (more capable, slower)
# - gemini-2.0-flash-exp (experimental, good balance)

# ============================================
# SCENARIO 2: AIPipe (Gemini via AIPipe proxy)
# ============================================
# To use this, set LLM_PROVIDER=aipipe above
# Get your token from: https://aipipe.org after login
# AIPIPE_TOKEN=YOUR_AIPIPE_TOKEN_HERE
# AIPIPE_GEMINI_MODEL=gemini-2.5-flash-lite

# ============================================
# SCENARIO 3: OpenAI API
# ============================================
# To use this, set LLM_PROVIDER=openai above
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-proj-YOUR_OPENAI_KEY_HERE
# OPENAI_MODEL=gpt-4o-mini

# ============================================
# SCENARIO 4: Local Ollama
# ============================================
# To use this, set LLM_PROVIDER=ollama above
# Make sure Ollama is running: ollama serve
# Note: Use host.docker.internal for Docker, localhost for local
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=qwen2.5-coder:1.5b

# Available Ollama models:
# - qwen2.5-coder:1.5b (optimized for code)
# - codellama (good for code generation)
# - deepseek-coder (specialized for coding)
# - mistral (general purpose)

# ============================================
# Common LLM Settings (applies to all providers)
# ============================================
# Temperature: 0.0 (deterministic) to 1.0 (creative)
LLM_TEMPERATURE=0.3

# Maximum tokens for LLM response
LLM_MAX_TOKENS=8192

# ============================================
# Optional - Database for logging
# ============================================
# Path to DuckDB database file for storing logs
DUCKDB_PATH=data/ai_coder.duckdb

